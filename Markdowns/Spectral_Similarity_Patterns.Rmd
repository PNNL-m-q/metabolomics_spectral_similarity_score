---
title: "Spectral_Similarity_Patterns"
author: "Degnan, David J"
date: "2023-06-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE)
library(data.table)
library(dplyr)
library(tidyr)
library(pheatmap)
library(ggplot2)
library(patchwork)
library(DT)
library(rsample)
library(randomForest)
library(recipes)
```

## Read all files 

```{r Create Max and Sum datasets}
# Load CoreMS Directory 
#CoreMS_Dir <- "/Users/degn400/Library/Group Containers/UBF8T346G9.OneDriveStandaloneSuite/OneDrive - PNNL.noindex/OneDrive - #PNNL/Desktop/MQ/MQ_Data/Final_Dataset"
#
## List files 
#MaxFiles <- list.files(file.path(CoreMS_Dir, "MAX"), full.names = T)
#SumFiles <- list.files(file.path(CoreMS_Dir, "SUM"), full.names = T)
#
## Function to extract data 
#pullData <- function(files) {
#  
#  return(do.call(bind_rows, lapply(files, function(x) {
#    
#    message(x)
#    
#    data <- fread(x) %>% 
#      dplyr::select(-c(`Retention Time`, `Retention Time Ref`, `Retention index`, `Retention index Ref`, `Retention Index Score`, Reason)) 
#    
#    numOmitted <- rowSums(is.na(data)) %>% .[. > 0] %>% length()
#    totalNum <- nrow(data)
#    
#    message(paste0("...Number omitted: ", numOmitted, " out of the total: ", totalNum, " which is ", round(numOmitted / totalNum, 4) * 100, "%"))
#    
#    data <- data %>% na.omit()
#    
#    # Random type fixes since data science the output is in scientific notation that 
#    # is sometimes read as a string. 
#    data$`Vicis Wave Hadges Distance` <- as.numeric(data$`Vicis Wave Hadges Distance`)
#    data$VW1 <- as.numeric(data$VW1)
#    data$`Symmetric Chi Squared Distance` <- as.numeric(data$`Symmetric Chi Squared Distance`)
#    data$`Kumar Johnson Distance` <- as.numeric(data$`Kumar Johnson Distance`)
#    data$`Kumar Johnson Divergence` <- as.numeric(data$`Kumar Johnson Divergence`)
#    
#    return(data)
#    
#  })))
#
#}
#
#MaxData <- pullData(MaxFiles)
#SumData <- pullData(SumFiles)
#
#fwrite(MaxData, "~/Downloads/MaxData.tsv", quote = F, row.names = F, sep = "\t")
#fwrite(SumData, "~/Downloads/SumData.tsv", quote = F, row.names = F, sep = "\t")
```

Here is a trelliscope display of the results: 

<iframe src="./Trelliscopes/SS_Scores/index.html" width=1000px height=600px data-external="1"> </iframe>>

## Correlation Score

#### Pearson Correlation: Max Score

```{r, MaxPearson, fig.height = 12, fig.width = 12}
MaxPearson <- fread("../Data/Max_Pearson.txt")
MP_Mat <- matrix(MaxPearson$Pearson, nrow = 74, ncol = 74)
colnames(MP_Mat) <- row.names(MP_Mat) <- MaxPearson$Score1 %>% unique()

HMA <- pheatmap(MP_Mat, main = "Max: Pearson Correlation")
HMA
```

There are 4 distinct clusters of scores using the max scaled data and a Pearson correlation. 

#### Pearson Correlation: Sum Score

```{r, SumPearson, fig.height = 12, fig.width = 12}
SumPearson <- fread("../Data/Sum_Pearson.txt")
SP_Mat <- matrix(SumPearson$Pearson, nrow = 74, ncol = 74)
colnames(SP_Mat) <- row.names(SP_Mat) <- MaxPearson$Score1 %>% unique()

HMB <- pheatmap(SP_Mat, main = "Sum: Pearson Correlation")
HMB
```

There are 4 distinct clusters of scores using the sum scaled data and a Pearson correlation. 
These are the same clusters as in the max scaled data.0

## Add T Statistic & Distributional Properties

Here, we are only interested in a t-statistic to measure the separation of score
distributions between true positives and true negatives. 

```{r}
## Add distribution properties 
#Sum <- fread("~/Downloads/SumData.tsv")
#
#SummaryStats <- do.call(bind_rows, lapply(ScoreMetadata$Score, function(x) {
#  c(
#    Score = x,
#    Sum_ScoreMin = Sum[[x]] %>% as.numeric() %>% min() %>% round(4),
#    Sum_ScoreMedian = Sum[[x]] %>% as.numeric() %>% median() %>% round(4),
#    Sum_ScoreMax = Sum[[x]] %>% as.numeric() %>% max() %>% round(4),
#    Max_ScoreMin = Max[[x]] %>% as.numeric() %>% min() %>% round(4),
#    Max_ScoreMedian = Max[[x]] %>% as.numeric() %>% median() %>% round(4),
#    Max_ScoreMax = Max[[x]] %>% as.numeric() %>% max() %>% round(4)
#  )
#}))
#ScoreMetadata <- ScoreMetadata %>% merge(SummaryStats, by = "Score")

## T-Statistics for Sum
#TstatsSum <- lapply(ScoreMetadata$Score, function(theScore) {
#  
#  # Extract the score sample
#  ScoreSample <- Sum %>% 
#    dplyr::select(!!theScore, Truth.Annotation) %>% 
#    dplyr::filter(Truth.Annotation != "")
#  
#  return(
#    t.test(
#      x = unlist(ScoreSample[ScoreSample$Truth.Annotation == "True.Positive", 1]) %>% as.numeric(),
#      y = unlist(ScoreSample[ScoreSample$Truth.Annotation == "True.Negative", 1]) %>% as.numeric(),
#      alternative = "two.sided"
#    )$statistic
#  )
#  
#}) %>% unlist()
#
## T-Statistics for Max
#TstatsMax <- lapply(ScoreMetadata$Score, function(theScore) {
#  
#  # Extract the score sample
#  ScoreSample <- Max %>% 
#    dplyr::select(!!theScore, Truth.Annotation) %>% 
#    dplyr::filter(Truth.Annotation != "")
#  
#  return(
#    t.test(
#      x = unlist(ScoreSample[ScoreSample$Truth.Annotation == "True.Positive", 1]) %>% as.numeric(),
#      y = unlist(ScoreSample[ScoreSample$Truth.Annotation == "True.Negative", 1]) %>% as.numeric(),
#      alternative = "two.sided"
#    )$statistic
#  )
#  
#}) %>% unlist()
#
#ScoreMetadata$TStatisticSum <- TstatsSum
#ScoreMetadata$TStatisticMax <- TstatsMax

ScoreMetadata <- fread("../Metadata/Score_Metadata.txt")

datatable(ScoreMetadata, options = list(scrollX = TRUE))
```

Descriptions: 

**Score:** The tested spectral similarity score

**Family:** The family of metrics the score comes from 

**Type:** Indicates whether the score is a similarity (ranges from 0-1 or -1 to 1 
fore directionality) or a distance (typically unbounded on one or both sides)

**Theoretical Bounds:** The bounds written as a categorical variable 

**Theoretical Bounds 2:** The bounds written in bracket format where a square bracket
is inclusive and a parentheses is not inclusive. 

**SumCluster:** The cluster number the score falls into for the sum data 

**MaxCluster:** The cluster number the score falls into for the max data 

**TStatisticSum:** The t-statistic between the true positives and true negatives
for a score in the sum data 

**TStatisticMax:** The t-statistic between the true positives and true negatives 
for a score in the max data 

**Sum_ScoreMin:** The score's minimum value in the sum dataset

**Sum_ScoreMedian:** The score's median value in the sum dataset

**Sum_ScoreMax:** The score's maximum value in the sum dataset 

**Max_ScoreMin:** The score's minimum value in the max dataset

**Max_ScoreMedian:** The score's median value in the max dataset

**Max_ScoreMax:** The score's maximum value in the max dataset 

**Overlap_Sum:** The overlap score (which is the propotion of true positives that 
overlap with the true negative/unknown distributions) for the sum dataset

**Overlap_Max:** The overlap score (which is the proportion of true positives that
overlap with the true negative/unknown distributions) for the max dataset

## Random Forest 

Data was split into 75% training and 25% testing datasets. Hyperparameters were 
fine tuned using the recipes and workflowsets R packages with leave one out 10-fold 
cross-validation repeated 5 times each for a total of 50 times. Random forest
models were fit using the ranger R package. 

#### Sum Model

The best fit model was: 

```{r}
SumModel <- readRDS("../Data/Sum_RF_Results.RDS")
SumModel[[1]]
```

The testing dataset yielded the following results: 

```{r}
SumModel[[2]]
```

Description:

```{r}
SumModel[[3]]
```

Description:

```{r}
SumModel[[4]]
```

#### Max Model

The best fit model was: 

```{r}
MaxModel <- readRDS("../Data/Max_RF_Results.RDS")
MaxModel[[1]]
```

The testing dataset yielded the following results: 

```{r}
MaxModel[[2]]
```

Description:

```{r}
MaxModel[[3]]
```

Description:

```{r}
MaxModel[[4]]
```

## Comparison

Given the high performance of both models, the most important variables for defining
clusters are: 

```{r}
merge(
  SumModel[[4]] %>% 
    mutate(Variable = gsub("Sum|_Sum", "", Variable)),
  MaxModel[[4]] %>% 
    dplyr::mutate(Variable = ifelse(Variable == "Max", "High", Variable),
                  Variable = gsub("Max|_Max", "", Variable),
                  Variable = ifelse(Variable == "High", "Max", Variable)),
  by = "Variable"
) %>% 
  rename(`Sum Importance` = Importance.x, `Max Importance` = Importance.y) %>%
  dplyr::arrange(-`Sum Importance`)
```

# Cluster Descriptions

**Sum Data:**

```{r}
ScoreMetadata %>%
  dplyr::group_by(SumCluster) %>%
  dplyr::rename(Cluster = SumCluster) %>%
  summarise(
    MeanTStat = mean(TStatisticSum),
    MeanOverlap = mean(Overlap_Sum),
    MeanMedian = mean(Sum_ScoreMedian),
    MeanMin = mean(Sum_ScoreMin),
    MeanMax = mean(Sum_ScoreMax),
    TheoreticalBounds_Left = sum(TheoreticalBounds == "Left"),
    TheoreticalBounds_Right = sum(TheoreticalBounds == "Right"),
    TheoreticalBounds_Both = sum(TheoreticalBounds == "Both"),
    TheoreticalBounds_None = sum(TheoreticalBounds == "Not"),
    Type_Distance = sum(Type == "Distance"),
    Type_Similarity = sum(Type == "Similarity"),
    `Chi Squared` = sum(Family == "Chi Squared"),
    Combination  = sum(Family == "Combination"),
    `Combined RI`  = sum(Family == "Combined RI"),
    Correlative  = sum(Family == "Correlative"),
    Fidelity  = sum(Family == "Fidelity"),
    `Inner Product`  = sum(Family == "Inner Product"),
    Intersection  = sum(Family == "Intersection"),
    `L1 Distance`  = sum(Family == "L1 Distance"),
    `L1 Metric`  = sum(Family == "L1 Metric"),
    `LP Distance`  = sum(Family == "LP Distance"),
    `Shannon Entropy`  = sum(Family == "Shannon's Entropy"),
    `Vicis Wave Hedges`  = sum(Family == "Vicis Wave Hedges")
  ) %>% DT::datatable(options = list(scrollX = T))
```

The "best" clusters are 1 and 2, just where Cluster 1 has higher scores for 
True Positives than True Negatives/Unknowns, and Cluster 2 has lower scores 
for True Positives than True Negatives/Unknown.

**Cluster 1 Diagnostics:** Highest T Stat, Lowest Overlap Score, majority of scores 
are bound at both ends and similarity metrics, most scores are inner product,
correlative, or intersection family

**Cluster 2 Diagnostics:** Lowest T Stat, 2nd best Overlap Score, mostly
bound to the left, primarily distance metrics. No obvious family patterns.

**Cluster 3 Diagnostics:** 3rd best Overlap score. All left-bound distance metrics.
Mostly the Chi Squared and Vicis Wave Hedges families. 

**Cluster 4 Diagnostics:** Worst overlap score. 5 distance families. No obvious
family patterns. 

**Max Data:**

```{r}
ScoreMetadata %>%
  dplyr::group_by(SumCluster) %>%
  dplyr::rename(Cluster = SumCluster) %>%
  summarise(
    MeanTStat = mean(TStatisticMax),
    MeanOverlap = mean(Overlap_Max),
    MeanMedian = mean(Max_ScoreMedian),
    MeanMin = mean(Max_ScoreMin),
    MeanMax = mean(Max_ScoreMax),
    TheoreticalBounds_Left = sum(TheoreticalBounds == "Left"),
    TheoreticalBounds_Right = sum(TheoreticalBounds == "Right"),
    TheoreticalBounds_Both = sum(TheoreticalBounds == "Both"),
    TheoreticalBounds_None = sum(TheoreticalBounds == "Not"),
    Type_Distance = sum(Type == "Distance"),
    Type_Similarity = sum(Type == "Similarity"),
    `Chi Squared` = sum(Family == "Chi Squared"),
    Combination  = sum(Family == "Combination"),
    `Combined RI`  = sum(Family == "Combined RI"),
    Correlative  = sum(Family == "Correlative"),
    Fidelity  = sum(Family == "Fidelity"),
    `Inner Product`  = sum(Family == "Inner Product"),
    Intersection  = sum(Family == "Intersection"),
    `L1 Distance`  = sum(Family == "L1 Distance"),
    `L1 Metric`  = sum(Family == "L1 Metric"),
    `LP Distance`  = sum(Family == "LP Distance"),
    `Shannon Entropy`  = sum(Family == "Shannon's Entropy"),
    `Vicis Wave Hedges`  = sum(Family == "Vicis Wave Hedges")
  ) %>% DT::datatable(options = list(scrollX = T))
```

Given the overlap score difference, we recommend using the sum method over the 
max method. 

Cluster patterns are the same as above.

## Figure 1

```{r}
MS <- fread("../Data/MS_Example/Query.txt")
MS1 <- fread("../Data/MS_Example/6-Phospho-D-gluconic acid.txt")
MS2 <- fread("../Data/MS_Example/beta-D-Fructose 6-phosphate.txt")
MS3 <- fread("../Data/MS_Example/D-Glucose 1-phosphate.txt")

make_ms_plot <- function(ms, compound) {
  
  data.frame(
    MZ = c(ms$MZ - 1e-9, ms$MZ, ms$MZ + 1e-9),
    Intensity = c(rep(0, nrow(ms)), ms$Intensity, rep(0, nrow(ms))),
    Abundance = c(rep(0, nrow(ms)), ms$Abundance, rep(0, nrow(ms)))
   ) %>%
    arrange(MZ) %>%
    ggplot(aes(x = MZ, y = Abundance)) + 
      geom_line() + 
      theme_bw() + 
      xlab(expression(italic("M/Z"))) +
      ggtitle(compound) + xlim(c(100, 550))
  
}


cosine_cor <- function(Ref, Query) {
  data <- merge(Ref, Query, by = "MZ", all = T) %>% 
    filter(!is.na(Abundance.x)) %>% 
    mutate(Intensity.y = ifelse(is.na(Intensity.y), 0, Intensity.y))
  lsa::cosine(data$Intensity.x, data$Intensity.y) %>%
  round(4)
}

make_ms_plot(MS, "Query Spectra is D-Glucose 1-phosphate") +
  make_ms_plot(MS1, paste("beta-D-Fructose 6-Phosphate, Cosine Correlation:", cosine_cor(MS1, MS))) +
  make_ms_plot(MS2, paste("6-phospho-D-gluconic Acid, Cosine Correlation:", cosine_cor(MS2, MS))) +
  make_ms_plot(MS3, paste("D-Glucose 1-phosphate, Cosine Correlation:", cosine_cor(MS3, MS))) +
  plot_annotation(tag_levels = "a")
```






