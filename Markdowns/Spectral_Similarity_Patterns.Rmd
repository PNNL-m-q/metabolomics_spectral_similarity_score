---
title: "Spectral_Similarity_Patterns"
author: "Degnan, David J"
date: "2023-05-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE)
library(data.table)
library(dplyr)
library(tidyr)
library(pheatmap)
library(ggplot2)
library(patchwork)
library(DT)
library(rsample)
library(randomForest)
library(recipes)
```

## Read all files 

```{r Create Max and Sum datasets}
# Load CoreMS Directory 
#CoreMS_Dir <- "/Users/degn400/Library/Group Containers/UBF8T346G9.OneDriveStandaloneSuite/OneDrive - PNNL.noindex/OneDrive - #PNNL/Desktop/MQ/MQ_Data/Final_Dataset"
#
## List files 
#MaxFiles <- list.files(file.path(CoreMS_Dir, "MAX"), full.names = T)
#SumFiles <- list.files(file.path(CoreMS_Dir, "SUM"), full.names = T)
#
## Function to extract data 
#pullData <- function(files) {
#  
#  return(do.call(bind_rows, lapply(files, function(x) {
#    
#    message(x)
#    
#    data <- fread(x) %>% 
#      dplyr::select(-c(`Retention Time`, `Retention Time Ref`, `Retention index`, `Retention index Ref`, `Retention Index Score`, Reason)) 
#    
#    numOmitted <- rowSums(is.na(data)) %>% .[. > 0] %>% length()
#    totalNum <- nrow(data)
#    
#    message(paste0("...Number omitted: ", numOmitted, " out of the total: ", totalNum, " which is ", round(numOmitted / totalNum, 4) * 100, "%"))
#    
#    data <- data %>% na.omit()
#    
#    # Random type fixes since data science the output is in scientific notation that 
#    # is sometimes read as a string. 
#    data$`Vicis Wave Hadges Distance` <- as.numeric(data$`Vicis Wave Hadges Distance`)
#    data$VW1 <- as.numeric(data$VW1)
#    data$`Symmetric Chi Squared Distance` <- as.numeric(data$`Symmetric Chi Squared Distance`)
#    data$`Kumar Johnson Distance` <- as.numeric(data$`Kumar Johnson Distance`)
#    data$`Kumar Johnson Divergence` <- as.numeric(data$`Kumar Johnson Divergence`)
#    
#    return(data)
#    
#  })))
#
#}
#
#MaxData <- pullData(MaxFiles)
#SumData <- pullData(SumFiles)
#
#fwrite(MaxData, "~/Downloads/MaxData.tsv", quote = F, row.names = F, sep = "\t")
#fwrite(SumData, "~/Downloads/SumData.tsv", quote = F, row.names = F, sep = "\t")
```

## Correlation Score

#### Pearson Correlation: Max Score

```{r, MaxPearson, fig.height = 12, fig.width = 12}
MaxPearson <- fread("../Data/Max_Pearson.txt")
MP_Mat <- matrix(MaxPearson$Pearson, nrow = 74, ncol = 74)
colnames(MP_Mat) <- row.names(MP_Mat) <- MaxPearson$Score1 %>% unique()

HMA <- pheatmap(MP_Mat, main = "Max: Pearson Correlation")
HMA
```

#### Pearson Correlation: Sum Score

```{r, SumPearson, fig.height = 12, fig.width = 12}
SumPearson <- fread("../Data/Sum_Pearson.txt")
SP_Mat <- matrix(SumPearson$Pearson, nrow = 74, ncol = 74)
colnames(SP_Mat) <- row.names(SP_Mat) <- MaxPearson$Score1 %>% unique()

HMB <- pheatmap(SP_Mat, main = "Sum: Pearson Correlation")
HMB
```

## Add T Statistic & Distributional Properties

Here, we are only interested in a t-statistic to measure the separation of score
distributions between true positives and true negatives. 

```{r}
## Add distribution properties 
#Sum <- fread("~/Downloads/SumData.tsv")
#
#SummaryStats <- do.call(bind_rows, lapply(ScoreMetadata$Score, function(x) {
#  c(
#    Score = x,
#    Sum_ScoreMin = Sum[[x]] %>% as.numeric() %>% min() %>% round(4),
#    Sum_ScoreMedian = Sum[[x]] %>% as.numeric() %>% median() %>% round(4),
#    Sum_ScoreMax = Sum[[x]] %>% as.numeric() %>% max() %>% round(4),
#    Max_ScoreMin = Max[[x]] %>% as.numeric() %>% min() %>% round(4),
#    Max_ScoreMedian = Max[[x]] %>% as.numeric() %>% median() %>% round(4),
#    Max_ScoreMax = Max[[x]] %>% as.numeric() %>% max() %>% round(4)
#  )
#}))
#ScoreMetadata <- ScoreMetadata %>% merge(SummaryStats, by = "Score")

## T-Statistics for Sum
#TstatsSum <- lapply(ScoreMetadata$Score, function(theScore) {
#  
#  # Extract the score sample
#  ScoreSample <- Sum %>% 
#    dplyr::select(!!theScore, Truth.Annotation) %>% 
#    dplyr::filter(Truth.Annotation != "")
#  
#  return(
#    t.test(
#      x = unlist(ScoreSample[ScoreSample$Truth.Annotation == "True.Positive", 1]) %>% as.numeric(),
#      y = unlist(ScoreSample[ScoreSample$Truth.Annotation == "True.Negative", 1]) %>% as.numeric(),
#      alternative = "two.sided"
#    )$statistic
#  )
#  
#}) %>% unlist()
#
## T-Statistics for Max
#TstatsMax <- lapply(ScoreMetadata$Score, function(theScore) {
#  
#  # Extract the score sample
#  ScoreSample <- Max %>% 
#    dplyr::select(!!theScore, Truth.Annotation) %>% 
#    dplyr::filter(Truth.Annotation != "")
#  
#  return(
#    t.test(
#      x = unlist(ScoreSample[ScoreSample$Truth.Annotation == "True.Positive", 1]) %>% as.numeric(),
#      y = unlist(ScoreSample[ScoreSample$Truth.Annotation == "True.Negative", 1]) %>% as.numeric(),
#      alternative = "two.sided"
#    )$statistic
#  )
#  
#}) %>% unlist()
#
#ScoreMetadata$TStatisticSum <- TstatsSum
#ScoreMetadata$TStatisticMax <- TstatsMax

ScoreMetadata <- fread("../Metadata/Score_Metadata.txt")

datatable(ScoreMetadata, options = list(scrollX = TRUE))
```

Descriptions: 

**Score:** The tested spectral similarity score

**Family:** The family of metrics the score comes from 

**Type:** Indicates whether the score is a similarity (ranges from 0-1 or -1 to 1 
fore directionality) or a distance (typically unbounded on one or both sides)

**Theoretical Bounds:** The bounds written as a categorical variable 

**Theoretical Bounds 2:** The bounds written in bracket format where a square bracket
is inclusive and a parentheses is not inclusive. 

**SumCluster:** The cluster number the score falls into for the sum data 

**MaxCluster:** The cluster number the score falls into for the max data 

**Sum_ScoreMin:** The score's minimum value in the sum dataset

**Sum_ScoreMedian:** The score's median value in the sum dataset

**Sum_ScoreMax:** The score's maximum value in the sum dataset 

**Max_ScoreMin:** The score's minimum value in the max dataset

**Max_ScoreMedian:** The score's median value in the max dataset

**Max_ScoreMax:** The score's maximum value in the max dataset 

**TStatisticSum:** The t-statistic between the true positives and true negatives
for a score in the sum data 

**TStatisticMax:** The t-statistic between the true positives and true negatives 
for a score in the max data 

## Random Forest 

#### Sum Model

Testing and training datasets.

```{r}
set.seed(339)

# Extract only Sum-Relevant Metadata
SumMetadata <- ScoreMetadata %>% 
  dplyr::select(SumCluster, Family, Type, TheoreticalBounds, Sum_ScoreMin, Sum_ScoreMedian, Sum_ScoreMax, TStatisticSum) %>%
  dplyr::mutate(
    SumCluster = factor(SumCluster, levels = 1:4),
    Family = as.factor(Family),
    Type = as.factor(Type), 
    TheoreticalBounds = as.factor(TheoreticalBounds)
  )

# Split data into testing and training 
sum_split <- rsample::initial_split(data = SumMetadata, prop = 3/4, strata = SumCluster, breaks = 4)
sum_train <- rsample::training(sum_split)
sum_test <- rsample::testing(sum_split)

# Create a recipe 
sum_recipe <- recipes::recipe(SumCluster ~ ., data = sum_train) 

# Designate a resampling scheme 
sum_folds <- rsample::vfold_cv(sum_train,
                               v = 10,
                               repeats = 5,
                               strata = SumCluster,
                               breaks = 4)

# Designate modeling engine
rf_engine <- parsnip::rand_forest(trees = tune::tune(), mtry = tune::tune()) %>%
  parsnip::set_engine("ranger", importance = "impurity") %>%
  parsnip::set_mode("classification")

# Designate test metrics 
test_metrics <- yardstick::metric_set(yardstick::accuracy)

# Specify number of trees based on a number of levels and m-try
rf_grid <- dials::grid_regular(dials::trees(), dials::mtry(range = c(1, ncol(SumMetadata)-1)), levels = 5) 

# Run the workflow - currently 1 random forest model
sum_wf <- workflowsets::workflow_set(
  preproc = list("nzv" = sum_recipe),
  models = list(rf = rf_engine)
)
wfid_names <- sum_wf$wflow_id
train_wfs <- sum_wf %>% 
  workflowsets::option_add(id = wfid_names[which(grepl("rf", wfid_names))], grid = rf_grid)

# Designate half of the cores to run 
ncores <- floor(parallel::detectCores(logical = TRUE)/2)

# Set the number of cores
cl <- parallel::makePSOCKcluster(ncores)

# Register cores to use
doParallel::registerDoParallel(cl)

# Run and resolve workflows 
sum_wf_complete <- sum_wf %>%
  workflowsets::workflow_map(resamples = sum_folds,
                             verbose = TRUE, 
                             seed = 339,
                             metrics = test_metrics)

# Stop parallel computing cores 
parallel::stopCluster(cl)
```

```{r}
# extract best fit parameters
best_rf_sum <- sum_wf_complete %>%
  workflowsets::extract_workflow_set_result(id = "nzv_rf") %>%
  tune::select_best(metric = "accuracy")

# fit to training
fitted_randf_sum_all <- sum_wf_complete %>%
  workflowsets::extract_workflow(id = "nzv_rf") %>%
  tune::finalize_workflow(best_rf_sum) %>%
  parsnip::fit(data = sum_train)


sum_cnmc_all <- predict(fitted_randf_sum_all, sum_test) %>%
  dplyr::bind_cols(predict(fitted_randf_sum_all, sum_test, type = "prob")) %>%
  dplyr::bind_cols(sum_test %>% dplyr::select(SumCluster)) %>%
  dplyr::rename(pred_class = .pred_class)

sum_cnmc_all_accuracy <- sum_cnmc_all %>%
  test_metrics(truth = SumCluster, estimate = pred_class)

fitted_randf_sum_all %>%
  workflows::extract_fit_parsnip() %>%
  vip::vi(scale = TRUE) %>%
  dplyr::arrange(dplyr::desc(Importance)) 
```





 
