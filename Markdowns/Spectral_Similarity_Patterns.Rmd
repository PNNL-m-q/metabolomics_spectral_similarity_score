---
title: "Spectral_Similarity_Patterns"
author: "Degnan, David J"
date: "2023-06-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE)
library(data.table)
library(dplyr)
library(tidyr)
library(pheatmap)
library(ggplot2)
library(patchwork)
library(DT)
library(rsample)
library(randomForest)
library(recipes)
```

## Read all files 

```{r Create Max and Sum datasets}
# Load CoreMS Directory 
#CoreMS_Dir <- "/Users/degn400/Library/Group Containers/UBF8T346G9.OneDriveStandaloneSuite/OneDrive - PNNL.noindex/OneDrive - #PNNL/Desktop/MQ/MQ_Data/Final_Dataset"
#
## List files 
#MaxFiles <- list.files(file.path(CoreMS_Dir, "MAX"), full.names = T)
#SumFiles <- list.files(file.path(CoreMS_Dir, "SUM"), full.names = T)
#
## Function to extract data 
#pullData <- function(files) {
#  
#  return(do.call(bind_rows, lapply(files, function(x) {
#    
#    message(x)
#    
#    data <- fread(x) %>% 
#      dplyr::select(-c(`Retention Time`, `Retention Time Ref`, `Retention index`, `Retention index Ref`, `Retention Index Score`, Reason)) 
#    
#    numOmitted <- rowSums(is.na(data)) %>% .[. > 0] %>% length()
#    totalNum <- nrow(data)
#    
#    message(paste0("...Number omitted: ", numOmitted, " out of the total: ", totalNum, " which is ", round(numOmitted / totalNum, 4) * 100, "%"))
#    
#    data <- data %>% na.omit()
#    
#    # Random type fixes since data science the output is in scientific notation that 
#    # is sometimes read as a string. 
#    data$`Vicis Wave Hadges Distance` <- as.numeric(data$`Vicis Wave Hadges Distance`)
#    data$VW1 <- as.numeric(data$VW1)
#    data$`Symmetric Chi Squared Distance` <- as.numeric(data$`Symmetric Chi Squared Distance`)
#    data$`Kumar Johnson Distance` <- as.numeric(data$`Kumar Johnson Distance`)
#    data$`Kumar Johnson Divergence` <- as.numeric(data$`Kumar Johnson Divergence`)
#    
#    return(data)
#    
#  })))
#
#}
#
#MaxData <- pullData(MaxFiles)
#SumData <- pullData(SumFiles)
#
#fwrite(MaxData, "~/Downloads/MaxData.tsv", quote = F, row.names = F, sep = "\t")
#fwrite(SumData, "~/Downloads/SumData.tsv", quote = F, row.names = F, sep = "\t")
```

Here is a trelliscope display of the results: 

<iframe src="./Trelliscopes/SS_Scores/index.html" width=1000px height=600px data-external="1"> </iframe>>

## Correlation Score

#### Pearson Correlation: Max Score

```{r, MaxPearson, fig.height = 12, fig.width = 12}
MaxPearson <- fread("../Data/Max_Pearson.txt")
MP_Mat <- matrix(MaxPearson$Pearson, nrow = 66, ncol = 66)
colnames(MP_Mat) <- row.names(MP_Mat) <- MaxPearson$Score1 %>% unique()

HMA <- pheatmap(MP_Mat, main = "Max: Pearson Correlation")
HMA
```

There are 4 distinct clusters of scores using the max scaled data and a Pearson correlation. 

#### Pearson Correlation: Sum Score

```{r, SumPearson, fig.height = 12, fig.width = 12}
SumPearson <- fread("../Data/Sum_Pearson.txt")
SP_Mat <- matrix(SumPearson$Pearson, nrow = 66, ncol = 66)
colnames(SP_Mat) <- row.names(SP_Mat) <- MaxPearson$Score1 %>% unique()

HMB <- pheatmap(SP_Mat, main = "Sum: Pearson Correlation")
HMB
```

There are 4 distinct clusters of scores using the sum scaled data and a Pearson correlation. 
These are the same clusters as in the max scaled data.0

## Add T Statistic & Distributional Properties

Here, we are only interested in a t-statistic to measure the separation of score
distributions between true positives and true negatives. 

```{r}
## Add distribution properties 
#Sum <- fread("~/Downloads/SumData.tsv")
#
#SummaryStats <- do.call(bind_rows, lapply(ScoreMetadata$Score, function(x) {
#  c(
#    Score = x,
#    Sum_ScoreMin = Sum[[x]] %>% as.numeric() %>% min() %>% round(4),
#    Sum_ScoreMedian = Sum[[x]] %>% as.numeric() %>% median() %>% round(4),
#    Sum_ScoreMax = Sum[[x]] %>% as.numeric() %>% max() %>% round(4),
#    Max_ScoreMin = Max[[x]] %>% as.numeric() %>% min() %>% round(4),
#    Max_ScoreMedian = Max[[x]] %>% as.numeric() %>% median() %>% round(4),
#    Max_ScoreMax = Max[[x]] %>% as.numeric() %>% max() %>% round(4)
#  )
#}))
#ScoreMetadata <- ScoreMetadata %>% merge(SummaryStats, by = "Score")

## T-Statistics for Sum
#TstatsSum <- lapply(ScoreMetadata$Score, function(theScore) {
#  
#  # Extract the score sample
#  ScoreSample <- Sum %>% 
#    dplyr::select(!!theScore, Truth.Annotation) %>% 
#    dplyr::filter(Truth.Annotation != "")
#  
#  return(
#    t.test(
#      x = unlist(ScoreSample[ScoreSample$Truth.Annotation == "True.Positive", 1]) %>% as.numeric(),
#      y = unlist(ScoreSample[ScoreSample$Truth.Annotation == "True.Negative", 1]) %>% as.numeric(),
#      alternative = "two.sided"
#    )$statistic
#  )
#  
#}) %>% unlist()
#
## T-Statistics for Max
#TstatsMax <- lapply(ScoreMetadata$Score, function(theScore) {
#  
#  # Extract the score sample
#  ScoreSample <- Max %>% 
#    dplyr::select(!!theScore, Truth.Annotation) %>% 
#    dplyr::filter(Truth.Annotation != "")
#  
#  return(
#    t.test(
#      x = unlist(ScoreSample[ScoreSample$Truth.Annotation == "True.Positive", 1]) %>% as.numeric(),
#      y = unlist(ScoreSample[ScoreSample$Truth.Annotation == "True.Negative", 1]) %>% as.numeric(),
#      alternative = "two.sided"
#    )$statistic
#  )
#  
#}) %>% unlist()
#
#ScoreMetadata$TStatisticSum <- TstatsSum
#ScoreMetadata$TStatisticMax <- TstatsMax

ScoreMetadata <- fread("../Metadata/Score_Metadata.txt")

datatable(ScoreMetadata, options = list(scrollX = TRUE))
```

Descriptions: 

**Score:** The tested spectral similarity score

**Family:** The family of metrics the score comes from 

**Type:** Indicates whether the score is a similarity (ranges from 0-1 or -1 to 1 
fore directionality) or a distance (typically unbounded on one or both sides)

**Theoretical Bounds:** The bounds written as a categorical variable 

**Theoretical Bounds 2:** The bounds written in bracket format where a square bracket
is inclusive and a parentheses is not inclusive. 

**SumCluster:** The cluster number the score falls into for the sum data 

**MaxCluster:** The cluster number the score falls into for the max data 

**TStatisticSum:** The t-statistic between the true positives and true negatives
for a score in the sum data 

**TStatisticMax:** The t-statistic between the true positives and true negatives 
for a score in the max data 

**Sum_ScoreMin:** The score's minimum value in the sum dataset

**Sum_ScoreMedian:** The score's median value in the sum dataset

**Sum_ScoreMax:** The score's maximum value in the sum dataset 

**Max_ScoreMin:** The score's minimum value in the max dataset

**Max_ScoreMedian:** The score's median value in the max dataset

**Max_ScoreMax:** The score's maximum value in the max dataset 

**Overlap_Sum:** The overlap score (which is the propotion of true positives that 
overlap with the true negative/unknown distributions) for the sum dataset

**Overlap_Max:** The overlap score (which is the proportion of true positives that
overlap with the true negative/unknown distributions) for the max dataset

## Random Forest 

Data was split into 75% training and 25% testing datasets. Hyperparameters were 
fine tuned using the recipes and workflowsets R packages with leave one out 10-fold 
cross-validation repeated 5 times each for a total of 50 times. Random forest
models were fit using the ranger R package. 

#### Sum Model

The best fit model was: 

```{r}
SumModel <- readRDS("../Data/Sum_RF_Results.RDS")
SumModel[[1]]
```

The testing dataset yielded the following results: 

```{r}
SumModel[[2]]
```




Description:

```{r}
SumModel[[3]]
```

Description:

```{r}
SumModel[[4]]
```

#### Max Model

The best fit model was: 

```{r}
MaxModel <- readRDS("../Data/Max_RF_Results.RDS")
MaxModel[[1]]
```

The testing dataset yielded the following results: 

```{r}
MaxModel[[2]]
```

Description:

```{r}
MaxModel[[3]]
```

Description:

```{r}
MaxModel[[4]]
```

## Comparison

Given the high performance of both models, the most important variables for defining
clusters are: 

```{r}
merge(
  SumModel[[4]] %>% 
    mutate(Variable = gsub("Sum|_Sum", "", Variable)),
  MaxModel[[4]] %>% 
    dplyr::mutate(Variable = ifelse(Variable == "Max", "High", Variable),
                  Variable = gsub("Max|_Max", "", Variable),
                  Variable = ifelse(Variable == "High", "Max", Variable)),
  by = "Variable"
) %>% 
  rename(`Sum Importance` = Importance.x, `Max Importance` = Importance.y) %>%
  dplyr::arrange(-`Sum Importance`)
```

# Cluster Descriptions

**Sum Data:**

```{r}
ScoreMetadata %>%
  dplyr::group_by(SumCluster) %>%
  dplyr::rename(Cluster = SumCluster) %>%
  summarise(
    MeanTStat = mean(TStatisticSum),
    MeanOverlap = mean(Overlap_Sum),
    MeanMedian = mean(Sum_ScoreMedian),
    MeanMin = mean(Sum_ScoreMin),
    MeanMax = mean(Sum_ScoreMax),
    TheoreticalBounds_Left = sum(TheoreticalBounds == "Left"),
    TheoreticalBounds_Right = sum(TheoreticalBounds == "Right"),
    TheoreticalBounds_Both = sum(TheoreticalBounds == "Both"),
    TheoreticalBounds_None = sum(TheoreticalBounds == "Not"),
    Type_Distance = sum(Type == "Distance"),
    Type_Similarity = sum(Type == "Similarity"),
    `Chi Squared` = sum(Family == "Chi Squared"),
    Combination  = sum(Family == "Combination"),
    `Combined RI`  = sum(Family == "Combined RI"),
    Correlative  = sum(Family == "Correlative"),
    Fidelity  = sum(Family == "Fidelity"),
    `Inner Product`  = sum(Family == "Inner Product"),
    Intersection  = sum(Family == "Intersection"),
    `L1 Distance`  = sum(Family == "L1 Distance"),
    `L1 Metric`  = sum(Family == "L1 Metric"),
    `LP Distance`  = sum(Family == "LP Distance"),
    `Shannon Entropy`  = sum(Family == "Shannon's Entropy"),
    `Vicis Wave Hedges`  = sum(Family == "Vicis Wave Hedges")
  ) %>% DT::datatable(options = list(scrollX = T))
```

The "best" clusters are 1 and 2, just where Cluster 1 has higher scores for 
True Positives than True Negatives/Unknowns, and Cluster 2 has lower scores 
for True Positives than True Negatives/Unknown.

**Cluster 1 Diagnostics:** Highest T Stat, Lowest Overlap Score, majority of scores 
are bound at both ends and similarity metrics, most scores are inner product,
correlative, or intersection family

**Cluster 2 Diagnostics:** Lowest T Stat, 2nd best Overlap Score, mostly
bound to the left, primarily distance metrics. No obvious family patterns.

**Cluster 3 Diagnostics:** 3rd best Overlap score. All left-bound distance metrics.
Mostly the Chi Squared and Vicis Wave Hedges families. 

**Cluster 4 Diagnostics:** Worst overlap score. 5 distance families. No obvious
family patterns. 

**Max Data:**

```{r}
ScoreMetadata %>%
  dplyr::group_by(SumCluster) %>%
  dplyr::rename(Cluster = SumCluster) %>%
  summarise(
    MeanTStat = mean(TStatisticMax),
    MeanOverlap = mean(Overlap_Max),
    MeanMedian = mean(Max_ScoreMedian),
    MeanMin = mean(Max_ScoreMin),
    MeanMax = mean(Max_ScoreMax),
    TheoreticalBounds_Left = sum(TheoreticalBounds == "Left"),
    TheoreticalBounds_Right = sum(TheoreticalBounds == "Right"),
    TheoreticalBounds_Both = sum(TheoreticalBounds == "Both"),
    TheoreticalBounds_None = sum(TheoreticalBounds == "Not"),
    Type_Distance = sum(Type == "Distance"),
    Type_Similarity = sum(Type == "Similarity"),
    `Chi Squared` = sum(Family == "Chi Squared"),
    Combination  = sum(Family == "Combination"),
    `Combined RI`  = sum(Family == "Combined RI"),
    Correlative  = sum(Family == "Correlative"),
    Fidelity  = sum(Family == "Fidelity"),
    `Inner Product`  = sum(Family == "Inner Product"),
    Intersection  = sum(Family == "Intersection"),
    `L1 Distance`  = sum(Family == "L1 Distance"),
    `L1 Metric`  = sum(Family == "L1 Metric"),
    `LP Distance`  = sum(Family == "LP Distance"),
    `Shannon Entropy`  = sum(Family == "Shannon's Entropy"),
    `Vicis Wave Hedges`  = sum(Family == "Vicis Wave Hedges")
  ) %>% DT::datatable(options = list(scrollX = T))
```

Given the overlap score difference, we recommend using the sum method over the 
max method. 

Cluster patterns are the same as above.

## Figure 1

```{r, fig.height = 4, fig.width = 10}
# Select an example
#Sum[Sum$`Sample name` == "UDN_BG_A_C_Biorec291064_M_012" & Sum$`Peak Index` == 13,] %>% 
#  arrange(-`Spectral Similarity Score`) %>% 
#  dplyr::select(`Compound Name`, `Spectral Similarity Score`, Truth.Annotation)

library(rhdf5)

#datasets <- h5ls("../Data/MS_Example/UDN_BG_A_C_Biorec291064_M_012.hdf5")
#datasets %>% filter(group == "/") %>% head(141) %>% dplyr::select(name) %>% unlist() %>% as.numeric() %>% sort() %>% .[13]
# RI is 7.675567

Query <- h5read("../Data/MS_Example/UDN_BG_A_C_Biorec291064_M_012.hdf5", "/7.675566666666667")
Alanine <- h5read("../Data/MS_Example/UDN_BG_A_C_Biorec291064_M_012.hdf5", "/7.675566666666667/L-alanine [major]")
Glycolic <- h5read("../Data/MS_Example/UDN_BG_A_C_Biorec291064_M_012.hdf5", "/7.675566666666667/glycolic acid")
Lactic <- h5read("../Data/MS_Example/UDN_BG_A_C_Biorec291064_M_012.hdf5", "/7.675566666666667/lactic acid")

make_ms_plot <- function(ms, compound, scale = FALSE) {
  
  res <- data.frame(
    MZ = c(ms$mz - 1e-9, ms$mz, ms$mz + 1e-9),
    Abundance = c(rep(0, length(ms$mz)), ms$abundance, rep(0, length(ms$mz)))
   ) %>%
    arrange(MZ) 
  
  if (scale) {res$Abundance <- (res$Abundance / max(res$Abundance) * 1000)}
  
  ggplot(res, aes(x = MZ, y = Abundance)) + 
      geom_line() + 
      theme_bw() + 
      xlab(expression(italic("M/Z"))) +
      ggtitle(compound) + xlim(c(100, 225))
  
}


Fig1 <- make_ms_plot(Query, "Query Spectra is L-Alanine", scale = T) +
  make_ms_plot(Glycolic, "Glycolic Acid, Cosine Correlation = 0.761") +
  make_ms_plot(Lactic, "Lactic Acid, Cosine Correlation = 0.588") +
  make_ms_plot(Alanine, "L-Alanine, Cosine Correlation = 0.165") +
  plot_annotation(tag_levels = "a")

Fig1
```


```{r}
Sum <- fread("~/Downloads/SumData.tsv")

Sub <- Sum %>%
  filter(Truth.Annotation %in% c("True.Positive", "True.Negative")) %>%
  select(Truth.Annotation, `Stein Scott Similarity Nist`, 
         `Weighted Cosine Correlation`, `Cosine Correlation`, `DWT Correlation`,
         `DFT Correlation`, `Canberra Metric`) %>%
  mutate(`DWT Correlation` = abs(`DWT Correlation`))

Sub %>%
  pivot_longer(2:7) %>%
  rename(Score = name, `Score Value` = value, Truth = Truth.Annotation) %>%
  mutate(
    Truth = ifelse(Truth == "True.Positive", "True Positive", "True Negative"),
    Truth = factor(Truth, levels = c("True Positive", "True Negative"))
  ) %>%
  ggplot(aes(x = Score, y = `Score Value`, fill = Truth)) +
    geom_boxplot() + 
    theme_bw() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
    scale_fill_manual(values = c("steelblue", "firebrick"))
```

```{r}
prop_thresh <- function(thresh) {
  Sub %>%
    pivot_longer(2:7) %>%
    filter(value >= thresh) %>%
    group_by(name) %>%
    summarize(
      TP.Proportion = sum(Truth.Annotation == "True.Positive") / 13486,
      TN.Proportion = sum(Truth.Annotation == "True.Negative") / 2575420
    ) %>%
    mutate(Threshold = thresh)
}

AllThresholds <- do.call(rbind, lapply((0:100)/100, prop_thresh))
```

```{r}
AllThresholds %>%
  rename(Score = name, `True Positive` = TP.Proportion, `True Negative` = TN.Proportion) %>%
  pivot_longer(2:3) %>%
  rename(Truth = name, Proportion = value, `Score Threshold` = Threshold) %>%
  mutate(Truth = factor(Truth, levels = c("True Positive", "True Negative"))) %>%
  filter(Score %in% c("Cosine Correlation", "Stein Scott Similarity Nist", "Canberra Metric")) %>%
  ggplot(aes(x = `Score Threshold`, y = Proportion, color = Score)) +
    geom_line() +
    theme_bw() +
    facet_wrap(.~Truth, scales = "free")
```

```{r}
SampleMetadata <- fread("../Metadata/Sample_Metadata.csv")

Sum %>%
  filter(Truth.Annotation == "True.Positive") %>%
  select(`Sample name`, `Stein Scott Similarity Nist`, `DFT Correlation`) %>%
  rename(`Sample Name` = `Sample name`) %>%
  merge(SampleMetadata, by = "Sample Name") %>%
  pivot_longer(2:3) %>%
  rename(Score = name, `Score Value` = value) %>%
  ggplot(aes(x = Score, y = `Score Value`, fill = `Sample Type`)) +
    geom_boxplot() +
    theme_bw()
```



## Figure 2

```{r}
ScoreMetadata <- fread("../Metadata/Score_Metadata.txt")
Sum <- fread("~/Downloads/SumData.tsv")
Sum$`Kumar Johnson Distance` <- as.numeric(Sum$`Kumar Johnson Distance`)
Sum$`Kumar Johnson Divergence` <- as.numeric(Sum$`Kumar Johnson Divergence`)

SubFams <- ScoreMetadata %>%
  mutate(Family = ifelse(Family == "Combined RI", "Combination", Family),
         Family = ifelse(Family == "L1 Metric", "L1 Distance", Family)) %>%
  group_by(Family) %>%
  slice_min(order_by = Overlap_Sum, n = 1, with_ties = F) %>%
  dplyr::select(Score, Family) %>%
  dplyr::rename(Metric = Score)

Fig2ToPlot <- Sum %>%
  dplyr::select(c(all_of(SubFams$Metric), Truth.Annotation)) %>%
  pivot_longer(all_of(SubFams$Metric)) %>%
  rename(Metric = name) %>%
  group_by(Metric) %>%
  rename(Value = value) %>%
  mutate(Truth = factor(ifelse(Truth.Annotation == "True.Positive", "True Positive", 
         ifelse(Truth.Annotation == "True.Negative", "True Negative", "Unknown")),
         levels = c("True Positive", "True Negative", "Unknown"))) %>%
  left_join(SubFams, by = "Metric") #%>%
  #mutate(Family = paste0(Family, "\n", Metric))

# Fix L1 Distance
Fig2ToPlot <- Fig2ToPlot %>%
  mutate(Value = ifelse(Family == "L1 Distance", log(Value + 1e-9), Value),
         Family = ifelse(Family == "L1 Distance", "Log(L1 Distance)", Family)) 
  
Fig2 <- ggplot(Fig2ToPlot, aes(x = Truth, y = Value)) +
  geom_boxplot() + theme_bw() + facet_wrap(.~Family, scales = "free")

Fig2
```

## Figure 4

```{r}
library(ggalluvial)

ScoreMetadata %>%
  mutate(Family = ifelse(Family == "Combined RI", "Combination", Family),
         Family = ifelse(Family == "L1 Metric", "L1 Distance", Family),
         SumCluster = as.factor(SumCluster),
         Family = as.factor(Family)) %>%
  rename(`Cluster Number` = SumCluster) %>%
  ggplot(aes(axis1 = Family, axis2 = `Cluster Number`)) + 
    geom_alluvium(aes(fill = Family), width = 1/12) +
    geom_stratum(width = 1/12, fill = "black", color = "grey") +
    geom_label(stat = "stratum", aes(label = after_stat(stratum))) +
    scale_fill_brewer(type = "qual", palette = "Paired") + 
    theme_minimal() + 
    scale_x_discrete(limits = c("Family", "Cluster"), expand = c(.05, .05)) +
    theme(legend.position = "none", axis.text.y = element_blank(), 
          axis.text.x = element_text(size = 16), panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(), panel.background = element_blank())
```

## Table 1

```{r}
ScoreMetadata %>%
  group_by(SumCluster) %>%
  summarise(
    TStat = mean(TStatisticSum),
    Overlap = mean(Overlap_Sum),
    ScoreMedian = mean(Sum_ScoreMedian)
  )

```

## Supplemental Table 1

```{r}
ScoreMetadata %>%
  group_by(SumCluster) %>%
  summarise(
    TStat = mean(TStatisticMax),
    Overlap = mean(Overlap_Max),
    ScoreMedian = mean(Max_ScoreMedian)
  )
```

## Figure 5

Also, last figure request: Can you make a variable importance plot (like the one at the bottom of this page https://www.tidymodels.org/start/case-study/) for the random forest model that you fit to predict cluster groups? I just feel we need a visual for that part of the analysis, and this is the only thing that made some sense to me. If you have other ideas though, please share!


```{r}
SumModel <- readRDS("../Data/Sum_RF_Results.RDS")
SumModel[[4]]$Variable <- c("T Statistic", "Overlap", "Median Value", "Family", 
                       "Min Value", "Theoretical Bounds", "Type", "Max Value")
SumModel[[4]]$Variable <- factor(SumModel[[4]]$Variable, levels = rev(SumModel[[4]]$Variable))

(ggplot(SumModel[[4]], aes(x = Importance, y = Variable)) + 
    geom_bar(stat = "identity") + theme(legend.position = "none") +
    theme_bw()) 
```







